{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda 3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have loaded 100 documents and 4893 words in total!\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# select the number of documents to parse\n",
    "Subset = 100 #10769\n",
    "filewithTFIDF = \"TFIDF_\"+str(Subset)+\"docs.npy\" # this is used later\n",
    "Distances = np.load(str(Subset)+'New_Doc_Distances.npy')\n",
    "\n",
    "N_Topics = 5\n",
    "\n",
    "FolderToParse = \"BoW_100random/\"#\"BagsOfWords/\"\n",
    "DocList = []\n",
    "for document in os.listdir( FolderToParse ):\n",
    "    # load documents\n",
    "    FileToLoad = FolderToParse + document\n",
    "    f = open(FileToLoad,'rb')\n",
    "    words = f.read().decode('ascii', 'ignore')\n",
    "    f.close()\n",
    "    words = words.split() # tokenize\n",
    "    DocList.append(words)\n",
    "    \n",
    "DocList = DocList[:Subset]\n",
    "# Load the names of species\n",
    "with open('list_of_species.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    Names = f.readlines()\n",
    "Names = [x.strip() for x in Names]\n",
    "\n",
    "# Create dictionary [ID to word]\n",
    "common_dictionary = Dictionary(DocList)\n",
    "# Create text to words mappings & count\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in DocList]\n",
    "print(\"We have loaded {0} documents and {1} words in total!\".format(len(DocList), len(common_dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeDistances(D, C1, C2):\n",
    "    \"\"\"\n",
    "        Function that, given two sets of indices C1, C2 and a matrix D with  \n",
    "        distances calculated for every pair, it computes the average distance.\n",
    "    \"\"\"\n",
    "    S=0\n",
    "    for i in range(len(C1)):\n",
    "        for j in range(len(C2)):\n",
    "            S += D[ C1[i], C2[j] ]\n",
    "    return S/(len(C1)*len(C2))\n",
    "     \n",
    "def EvalClustering(D, Clustering):\n",
    "    \"\"\"\n",
    "        Function that, given a set clusters and a matrix D with distances calculated \n",
    "        for every pair of points, evaluates the accuracy of the partition.\n",
    "        Intra : the average distance for points within one cluster\n",
    "        Inter : the average distance between points from different clusters.\n",
    "    \"\"\"\n",
    "    N_K = len(Clustering)\n",
    "    ClusterDist = np.zeros( (N_K,N_K) )\n",
    "    for c1 in range(N_K):\n",
    "        if len(Clustering[c1])>0:\n",
    "            for c2 in range(c1,N_K):\n",
    "                #first we compute the intra-cluster distance\n",
    "                if len(Clustering[c2])>0:\n",
    "                    ClusterDist[c1,c2] = ComputeDistances(D, list(Clustering[c1]), list(Clustering[c2]))\n",
    "    # evaluate\n",
    "    intra = np.mean(np.diag(ClusterDist))\n",
    "    print('Mean Within-Cluster distance = {0:.3f}.'.format(intra))\n",
    "    inter = np.sum(np.triu(ClusterDist,1))*2/(N_K-1)/N_K\n",
    "    print('Mean Inter-Cluster distance = {0:.3f}.'.format(inter))\n",
    "    return intra, inter, ClusterDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LDA by gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA is complete!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=N_Topics, alpha='asymmetric', random_state=1)\n",
    "# Now produce probabilities based on the Corpus\n",
    "LDAvectors = []\n",
    "for i in range(len(DocList)):\n",
    "    # first we translate using the dictionary that we have already\n",
    "    temp = [ common_dictionary.doc2bow(text.split()) for text in DocList[i] ]\n",
    "    vector = lda[temp[0]]\n",
    "    LDAvectors.append( vector )\n",
    "print('LDA is complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Evaluate - gensim LDA with BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Within-Cluster distance = 4.882.\n",
      "Mean Inter-Cluster distance = 6.838.\n"
     ]
    }
   ],
   "source": [
    "#LDAvectors = np.load(\"LDAvectors_gensim_\"+str(Subset)+\".npy\")\n",
    "\n",
    "# Clustering #\n",
    "ClustersLDA = { i:[] for i in range(N_Topics)}# initialize the dictictionary of clusters\n",
    "ClustersNames = { i:[] for i in range(N_Topics)} \n",
    "Labels = []\n",
    "for i in range(len(DocList)):\n",
    "    distr = [ x[1] for x in LDAvectors[i]]\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = distr.index(max(distr))\n",
    "    ClustersLDA[label].append(i)\n",
    "    ClustersNames[label].append(Names[i])\n",
    "    Labels.append( label )\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersLDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Evaluate - gensim LDA with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=10, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 4.882.\n",
      "Mean Inter-Cluster distance = 5.061.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# load data (again)\n",
    "#X =  np.load(\"LDAvectors_gensim_\"+str(Subset)+\".npy\")\n",
    "\n",
    "# possibly needs transform\n",
    "X = LDAvectors\n",
    "X = np.array([[P[1] for P in Z] for Z in X])\n",
    "#print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "\n",
    "km = KMeans(n_clusters=N_Topics, init='k-means++', max_iter=100, n_init=1, random_state=10)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X)\n",
    "\n",
    "# create the clusters\n",
    "Clusters = { i:[] for i in range(N_Topics)}\n",
    "Labels = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    Clusters[Labels[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, Clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LDA by Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_features: 4893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda 3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda 3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:532: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
      "             learning_offset=10.0, max_doc_update_iter=100, max_iter=500,\n",
      "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=5,\n",
      "             perp_tol=0.1, random_state=50, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "X = np.load(filewithTFIDF)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "LDA_SKL = LatentDirichletAllocation(n_topics = N_Topics, max_iter=500, random_state=50)\n",
    "LDA_SKL.fit(X)\n",
    "print(\"Clustering sparse data with %s\" % LDA_SKL)\n",
    "# get doc-topic distributions\n",
    "LDA_SKLvectors = LDA_SKL.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Evaluate scikit LDA with BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Within-Cluster distance = 6.444.\n",
      "Mean Inter-Cluster distance = 6.020.\n"
     ]
    }
   ],
   "source": [
    "# Clustering - Black and white approach, as before\n",
    "ClustersSKL = { i:[] for i in range(N_Topics)}\n",
    "LabelsSKL = []\n",
    "for i in range(len(DocList)):\n",
    "    distr = list(LDA_SKLvectors[i])\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = distr.index(max(distr))\n",
    "    ClustersSKL[label].append(i)\n",
    "    LabelsSKL.append( label )\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersSKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Evaluate scikit LDA with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=10, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 6.444.\n",
      "Mean Inter-Cluster distance = 6.692.\n"
     ]
    }
   ],
   "source": [
    "X = LDA_SKLvectors\n",
    "\n",
    "km = KMeans(n_clusters=N_Topics, init='k-means++', max_iter=100, n_init=1, random_state=10)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X)\n",
    "\n",
    "# create the clusters\n",
    "Clusters = { i:[] for i in range(N_Topics)}\n",
    "Labels = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    Clusters[Labels[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, Clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Benchmark technique: K-Means on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_features: 4893\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=10, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 1.307.\n",
      "Mean Inter-Cluster distance = 9.162.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X =  np.load(filewithTFIDF)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "km = KMeans(n_clusters=N_Topics, init='k-means++', max_iter=100, n_init=1, random_state=10)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X)\n",
    "\n",
    "# create the clusters\n",
    "ClustersKM = { i:[] for i in range(N_Topics)}\n",
    "LabelsKM = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    ClustersKM[LabelsKM[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersKM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Advanced approach: SVD + K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_features: 50\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=10, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 1.309.\n",
      "Mean Inter-Cluster distance = 13.452.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Dimensionality reduction\n",
    "svd = TruncatedSVD(50)\n",
    "# we use the same X as before\n",
    "X = svd.fit_transform(X)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "km = KMeans(n_clusters=N_Topics, init='k-means++', max_iter=100, n_init=1, random_state=10)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X)\n",
    "\n",
    "# create the clusters\n",
    "ClustersKM = { i:[] for i in range(N_Topics)}\n",
    "LabelsKM = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    ClustersKM[LabelsKM[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersKM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Our LDA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the big loop...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e3522288cf11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_W\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdoc_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_number\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_number\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[1;31m###for doc_length in range(len(Z[doc_number])):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;31m###    n[v] += (X_number[doc_number][ doc_length]==v) and (Z[doc_number][doc_length] ==k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Implement LDA\n",
    "# Number matrix (Replacing words in documents with word IDs)\n",
    "from time import time\n",
    "N_K = 5 # N_Topics # set the number of topics\n",
    "N_D = len(DocList)\n",
    "corpus=np.unique(np.concatenate(DocList),axis=0)\n",
    "N_W = corpus.shape[0] # words in the vocabulary\n",
    "\n",
    "# SELECT #iterations\n",
    "T=100\n",
    "\n",
    "X_number = np.copy(DocList)\n",
    "for doc_number in range(X_number.shape[0]):\n",
    "    for doc_length in range(len(X_number[doc_number])):\n",
    "        X_number[doc_number][doc_length]=  np.where(corpus==X_number[doc_number][doc_length])[0][0]\n",
    "        \n",
    "# Dirichlet priors\n",
    "alpha = 1 # Choice of alpha affects document clustering \n",
    "gamma = 1\n",
    "\n",
    "#Z = np.copy(X_number)\n",
    "#for doc_number in range(Z.shape[0]):\n",
    "#    for doc_length in range(len(Z[doc_number])):\n",
    "#        Z[doc_number][doc_length]= np.random.randint(N_K)\n",
    "        \n",
    "Z = []#[np.array(N_D, dtype=object)]\n",
    "for doc in range(N_D):\n",
    "    Z.append( np.random.randint(N_K, size=len(DocList[doc])) )\n",
    "        \n",
    "# Pi := document topic distribution\n",
    "Pi = np.zeros([N_D, N_K])\n",
    "for i in range(N_D):\n",
    "    Pi[i] = np.random.dirichlet(alpha*np.ones(N_K))\n",
    "\n",
    "A = Pi\n",
    "#print(A)\n",
    "\n",
    "# B := word topic distribution\n",
    "B = np.zeros([N_K, N_W])\n",
    "for k in range(N_K):\n",
    "    B[k] = np.random.dirichlet(gamma*np.ones(N_W))\n",
    "t0 = time()    \n",
    "print(\"Starting the big loop...\")    \n",
    "for iterations in range(T):  #Need at least 1000 iterations for Gibbs sampling to work!\n",
    "\n",
    "    # Updating Z matrix\n",
    "    for doc_number in range(N_D):     \n",
    "        for doc_length in range(len(Z[doc_number])):     \n",
    "            # Calculate params for Z\n",
    "            p_iv = np.exp(np.log(Pi[doc_number]) + np.log( B[:, X_number[doc_number][ doc_length]] ))\n",
    "            p_iv /= np.sum(p_iv)\n",
    "\n",
    "             # Resample word topic assignment Z\n",
    "            Z[doc_number][doc_length] = np.random.multinomial(1, p_iv).argmax()\n",
    "    # Updating Pi   \n",
    "    for i in range(N_D):\n",
    "        # Gather sufficient statistics\n",
    "        ###m = np.zeros(N_K)\n",
    "        ###for k in range(N_K):\n",
    "        ###    m[k] = np.sum(Z[i] == k)\n",
    "        \n",
    "        m = np.array( [np.sum(Z[i] == k) for k in range(N_K)] )\n",
    "        # Resample doc topic dist.\n",
    "        Pi[i, :] = np.random.dirichlet(alpha + m)\n",
    "        \n",
    "    #Updating B\n",
    "    for k in range(N_K):\n",
    "        #print(k)\n",
    "        n = np.zeros(N_W) \n",
    "    \n",
    "        #Gather statistics       \n",
    "        for v in range(N_W):\n",
    "            for doc_number in range(N_D):\n",
    "                n[v] = len([ x for x in np.where(X_number[doc_number] == v) if Z[doc_number][x] ==k ])\n",
    "                ###for doc_length in range(len(Z[doc_number])):\n",
    "                ###    n[v] += (X_number[doc_number][ doc_length]==v) and (Z[doc_number][doc_length] ==k)\n",
    "        \n",
    "        # Resample word topic distribution\n",
    "        B[k,:] = np.random.dirichlet(gamma+n)\n",
    "    #progress check\n",
    "    if (iterations-1)%10==0:\n",
    "        print(\"More than {0} % is completed! Rate = {1}\".format(100*iterations/T, (time()-t0)/iterations))\n",
    "print('LDA is complete! Total time = {0}'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Our LDA with BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Within-Cluster distance = 6.307.\n",
      "Mean Inter-Cluster distance = 7.007.\n"
     ]
    }
   ],
   "source": [
    "Pi = np.load(\"prob100it5topgeorge.npy\")\n",
    "# Clustering #\n",
    "ClustersOUR = { i:[] for i in range(N_Topics)}# initialize the dictictionary of clusters\n",
    "for i in range(len(DocList)):\n",
    "    #distr = [ x[1] for x in Pi[i]]\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = np.argmax(Pi[i])\n",
    "    ClustersOUR[label].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersOUR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Our LDA with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering LDA data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=1, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 5.132.\n",
      "Mean Inter-Cluster distance = 5.799.\n"
     ]
    }
   ],
   "source": [
    "X = Pi\n",
    "km = KMeans(n_clusters=N_Topics, init='k-means++', max_iter=100, n_init=1, random_state=1)\n",
    "print(\"Clustering LDA data with %s\" % km)\n",
    "km.fit(X)\n",
    "\n",
    "# create the clusters\n",
    "ClustersXX = { i:[] for i in range(N_Topics)}\n",
    "LabelsXX = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    ClustersXX[LabelsXX[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clustering with random assigning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Within-Cluster distance = 6.283.\n",
      "Mean Inter-Cluster distance = 7.337.\n"
     ]
    }
   ],
   "source": [
    "ClustersRAND = { i:[] for i in range(N_Topics)}# initialize the dictictionary of clusters\n",
    "for i in range(len(DocList)):\n",
    "    #distr = [ x[1] for x in Pi[i]]\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = np.random.randint(N_Topics)\n",
    "    ClustersRAND[label].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersRAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of different LDA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KLdivergence(P,Q):\n",
    "    #import numpy as np\n",
    "    n = P.shape[0]\n",
    "    total = []\n",
    "    for subject in range(n):\n",
    "        D = 0\n",
    "        for x in range(P.shape[1]):\n",
    "            D +=P[subject][x]*np.log( P[subject][x]/Q[subject][x] )\n",
    "        total.append(D)\n",
    "    return total #np.mean(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "X = np.load(filewithTFIDF)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "N_Topics = 5\n",
    "LDA_SKL = LatentDirichletAllocation(n_topics = N_Topics, max_iter=1100, random_state=0)\n",
    "LDA_SKL.fit(X)\n",
    "LDA_skl1 = LDA_SKL.transform(X)\n",
    "\n",
    "LDA_SKL = LatentDirichletAllocation(n_topics = N_Topics, max_iter=1100, random_state=30)\n",
    "LDA_SKL.fit(X)\n",
    "LDA_skl2 = LDA_SKL.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LDAour1 = np.load(\"prob100it5topgeorge.npy\")\n",
    "LDAour2 = np.load(\"prob100it5topcristiana.npy\")\n",
    "\n",
    "#LDAgen1 = LDAvectors\n",
    "#LDAgen2 = LDAvectors\n",
    "#LDAgen1 = np.array([[P[1] for P in Z] for Z in LDAgen1])\n",
    "#LDAgen2 = np.array([[P[1] for P in Z] for Z in LDAgen2])\n",
    "\n",
    "compare = [LDAour1, LDAour2, LDAgen1, LDAgen2, LDA_skl1, LDA_skl2]\n",
    "\n",
    "DL_All = np.zeros((len(compare), len(compare)))\n",
    "for i in range(len(compare)):\n",
    "    for j in range(len(compare)):\n",
    "        DL_All[i,j] = np.around(np.mean( KLdivergence(compare[i], compare[j])),decimals=3 )\n",
    "DL_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print for latex\n",
    "#f = open('Table_KL.txt','w')\n",
    "names = [\"ourLDA-1\", \"ourLDA-2\", \"gensimLDA-1\" ,\"gensimLDA-2\", \"skLDA-1\", \"skLDA-2\"]\n",
    "for i in range(len(compare)):\n",
    "    line = \"\\\\textit{\"+ names[i] +\"}\"\n",
    "    for j in range(len(compare)):\n",
    "        line += \" & \" + str(DL_All[i,j])\n",
    "    line += \"\\\\\\\\ \"\n",
    "#     f.write(\"$ {0} $ & {1} & {2} & {3:.2f} & {4} & {5:.3f} & {6} \\\\\\\\ \\n\".format( shortname, N, M, np.mean(degrees), concomps, largest, diam ))\n",
    "    print(line )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda 3]",
   "language": "python",
   "name": "Python [Anaconda 3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
