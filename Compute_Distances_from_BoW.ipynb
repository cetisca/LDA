{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Mining Tools\n",
    "\n",
    "This Python document does some fundamental handling of text data. It starts by loading a set of documents from Bag-of-words (preprocessing is required!), then creates lists with term-documents represantation, calculates the standard tf-idf matrix (without normalizing for tf) and computes the doc-doc (euclidean) distances based on tf-idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from scipy.sparse import dia_matrix, csr_matrix, csc_matrix\n",
    "from time import time\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FolderToParse = \"BoW_100random/\"# \"BagsOfWords/\" # \n",
    "DocList = []\n",
    "totalwords = 0 # vaariable for sanity check\n",
    "for document in os.listdir( FolderToParse ):\n",
    "    # load documents\n",
    "    FileToLoad = FolderToParse + document\n",
    "    f = open(FileToLoad,'rb')\n",
    "    words = f.read().decode('ascii', 'ignore')\n",
    "    f.close()\n",
    "    words = words.split()\n",
    "    DocList.append(words)\n",
    "    # counter for sanity checks\n",
    "    totalwords += len(words)\n",
    "\n",
    "Vocabulary = np.unique(np.concatenate(DocList))\n",
    "N_D = len(DocList) # number of documents\n",
    "N_V = len(Vocabulary)  # number of vocabulary - all available words\n",
    "\n",
    "print(\"We have {0} documents and {1} words in total!\".format(N_D, N_V))\n",
    "print(\"Total number of words found= %d\" % totalwords )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Term-Document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TDM = csr_matrix((N_V,N_D))\n",
    "TDM = np.zeros((N_V,N_D)) # we start by a full matrix and then transform it\n",
    "t0 = time()\n",
    "# create doc-word list of lists\n",
    "for doc in range(N_D):\n",
    "    temp = np.unique( DocList[doc] ) # get the different words on this document\n",
    "    for i in range( len(temp) ):\n",
    "        word = temp[i] \n",
    "        count = len([ x for x in DocList[doc] if x == word])\n",
    "        # we must get the index of this word in the (total) corpus\n",
    "        TDM[ np.where(Vocabulary == word) , doc] = count\n",
    "    # progress check\n",
    "    if ((doc+1) % 500) == 0 :\n",
    "        print('More than {0} documents have been processed! Rate = {1}'.format(doc+1, (time()-t0)/doc))\n",
    "\n",
    "# sanity check\n",
    "for doc in range(N_D):\n",
    "    if sum(TDM[:,doc])!=len(DocList[doc]):\n",
    "        print(\"Doc-{0} has a problem!\".format(doc))\n",
    "        \n",
    "TDM = csr_matrix(TDM)\n",
    "print(\"The Term-Doc matrix is {0:.2f}% dense.\".format(csr_matrix.count_nonzero(TDM)/np.prod(TDM.shape)*100))\n",
    "# Save ?\n",
    "filetosave = 'Term_Doc_Matrix_'+str(N_D)+'.npz'\n",
    "save_npz(filetosave,TDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first compute the frequencies of each word among documents - sum of rows in TDM :\n",
    "IDF = TDM.sum(axis=1)\n",
    "IDF = np.array( [np.log(N_D/tf) for tf in IDF] ) # trick to get rid of the (1,N_V) dimension thing\n",
    "IDF = [x[0][0] for x in IDF]\n",
    "# create sparse diag matrix :\n",
    "spdiag = csr_matrix( (IDF, range(N_V),range(N_V+1)) )\n",
    "TF_IDF = TDM.transpose()*spdiag\n",
    "# Save ?\n",
    "filetosave = 'TFIDF_'+str(N_D)+'.npy'\n",
    "np.save(filetosave,TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute doc-doc distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Distances = pdist(TF_IDF.todense(), 'euclidean')\n",
    "Distances = squareform(Distances) # this is a full matrix (and heavy!)\n",
    "# Save ?\n",
    "filetosave = 'DocDistances_'+str(N_D)+'.npy'\n",
    "np.save(filetosave, Distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### an alternative (and slower?) way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import norm\n",
    "Distances = np.empty((N_D,N_D))\n",
    "Distances = []\n",
    "t0=time()\n",
    "for i in range(2):\n",
    "#    Distances[i][i+1:] = [ norm(TF_IDF[:,i] - TF_IDF[:,j]) for j in range(i+1,N_D)]\n",
    "    Distances.append( [ norm(TF_IDF[:,i] - TF_IDF[:,j]) for j in range(i+1,N_D)] )\n",
    "#print((time()-t0)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeDistances(D, C1, C2):\n",
    "    \"\"\"\n",
    "        Function that, given two sets of indices C1, C2 and a matrix D with  \n",
    "        distances calculated for every pair, it computes the average distance.\n",
    "    \"\"\"\n",
    "    S=0\n",
    "    for i in range(len(C1)):\n",
    "        for j in range(len(C2)):\n",
    "            S += D[ C1[i], C2[j] ]\n",
    "    return S/(len(C1)*len(C2))\n",
    "     \n",
    "def EvalClustering(D, Clustering):\n",
    "    \"\"\"\n",
    "        Function that, given a set clusters and a matrix D with distances calculated \n",
    "        for every pair of points, evaluates the accuracy of the partition.\n",
    "        Intra : the average distance for points within one cluster\n",
    "        Inter : the average distance between points from different clusters.\n",
    "    \"\"\"\n",
    "    N_K = len(Clustering)\n",
    "    ClusterDist = np.zeros( (N_K,N_K) )\n",
    "    for c1 in range(N_K):\n",
    "        if len(Clustering[c1])>0:\n",
    "            for c2 in range(c1,N_K):\n",
    "                #first we compute the intra-cluster distance\n",
    "                if len(Clustering[c2])>0:\n",
    "                    ClusterDist[c1,c2] = ComputeDistances(D, list(Clustering[c1]), list(Clustering[c2]))\n",
    "    # evaluate\n",
    "    intra = np.mean(np.diag(ClusterDist))\n",
    "    print('Mean Within-Cluster distance = {0:.3f}.'.format(intra))\n",
    "    inter = np.sum(np.triu(ClusterDist,1))*2/(N_K-1)/N_K\n",
    "    print('Mean Inter-Cluster distance = {0:.3f}.'.format(inter))\n",
    "    return intra, inter, ClusterDist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda 3]",
   "language": "python",
   "name": "Python [Anaconda 3]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
