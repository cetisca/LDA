{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 documents and 4893 words in total!\n",
      "Total number of words found= 12557\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "#from sklearn.metrics.pairwise import pairwise_distances\n",
    "import os\n",
    "\n",
    "FolderToParse = \"BoW_100random/\"# \"BagsOfWords/\" # \n",
    "DocList = []\n",
    "totalwords = 0\n",
    "for document in os.listdir( FolderToParse ):\n",
    "    # load documents\n",
    "    FileToLoad = FolderToParse + document\n",
    "    f = open(FileToLoad,'rb')\n",
    "    words = f.read().decode('ascii', 'ignore')\n",
    "    f.close()\n",
    "    words = words.split()\n",
    "    DocList.append(words)\n",
    "    # counter for sanity checks\n",
    "    totalwords += len(words)\n",
    "X_words = DocList#[:100]\n",
    "\n",
    "corpus=np.unique(np.concatenate(X_words))\n",
    "N_D = len(X_words) # number of documents\n",
    "N_V = len(corpus)  # number of vocabulary - all available words\n",
    "N_K = 10 # set the number of topics\n",
    "print(\"We have {0} documents and {1} words in total!\".format(N_D, N_V))\n",
    "print(\"Total number of words found= %d\" % totalwords )\n",
    "def ComputeDistances(D, C1, C2):\n",
    "    \"\"\"\n",
    "        Function that, given two sets of indices C1, C2 and a matrix D with  \n",
    "        distances calculated for every pair, it computes the average distance.\n",
    "    \"\"\"\n",
    "    S=0\n",
    "    for i in range(len(C1)):\n",
    "        for j in range(len(C2)):\n",
    "            S += D[ C1[i], C2[j] ]\n",
    "    return S/(len(C1)*len(C2))\n",
    "     \n",
    "def EvalClustering(D, Clustering):\n",
    "    \"\"\"\n",
    "        Function that, given a set clusters and a matrix D with distances calculated \n",
    "        for every pair of points, evaluates the accuracy of the partition.\n",
    "        Intra : the average distance for points within one cluster\n",
    "        Inter : the average distance between points from different clusters.\n",
    "    \"\"\"\n",
    "    N_K = len(Clustering)\n",
    "    ClusterDist = np.zeros( (N_K,N_K) )\n",
    "    for c1 in range(N_K):\n",
    "        if len(Clustering[c1])>0:\n",
    "            for c2 in range(c1,N_K):\n",
    "                #first we compute the intra-cluster distance\n",
    "                if len(Clustering[c2])>0:\n",
    "                    ClusterDist[c1,c2] = ComputeDistances(D, list(Clustering[c1]), list(Clustering[c2]))\n",
    "    # evaluate\n",
    "    intra = np.mean(np.diag(ClusterDist))\n",
    "    print('Mean Within-Cluster distance = {0:.3f}.'.format(intra))\n",
    "    inter = np.sum(np.triu(ClusterDist,1))*2/(N_K-1)/N_K\n",
    "    print('Mean Inter-Cluster distance = {0:.3f}.'.format(inter))\n",
    "    return intra, inter, ClusterDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Term-Doc matrix is 2.57% dense.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# initialize term - document matrix\n",
    "#TDM = csr_matrix((N_V,N_D))\n",
    "TDM = np.zeros((N_V,N_D)) # we start by a full matrix and then transform it\n",
    "#TDM = [] \n",
    "t0 = time()\n",
    "# create doc-word list of lists\n",
    "for doc in range(N_D):\n",
    "    temp = np.unique( X_words[doc] ) # get the different words on this document\n",
    "    #temp2 = np.zeros((N_V,1))\n",
    "    for i in range( len(temp) ):\n",
    "        word = temp[i] \n",
    "        count = len([ x for x in X_words[doc] if x == word])\n",
    "        # we must get the index of this word in the (total) corpus\n",
    "        TDM[ np.where(corpus == word) , doc] = count\n",
    "        #temp2[ np.where(corpus == word) ] = count\n",
    "    #TDM.append(temp2)\n",
    "    # progress check\n",
    "    if ((doc+1) % 500) == 0 :\n",
    "        print('More than {0} documents have been processed! Rate = {1}'.format(doc+1, (time()-t0)/doc))\n",
    "\n",
    "# sanity check\n",
    "for doc in range(N_D):\n",
    "    if sum(TDM[:,doc])!=len(X_words[doc]):\n",
    "        print(\"Doc-{0} has a problem!\".format(doc))\n",
    "TDM = csr_matrix(TDM)\n",
    "print(\"The Term-Doc matrix is {0:.2f}% dense.\".format(csr_matrix.count_nonzero(TDM)/np.prod(TDM.shape)*100))\n",
    "#print(\"The Term-Doc matrix is {0:.2f}% dense.\".format(np.count_nonzero(TDM)/np.prod(TDM.shape)*100))\n",
    "# SAVE IT\n",
    "# scipy.sparse\n",
    "#scipy.sparse.save_npz('Term_Doc_Matrix_All.npz',TDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf matrix\n",
    "TF_IDF = np.zeros((N_V,N_D))\n",
    "# first compute the frequencies of each word among documents - sum of rows in TDM\n",
    "IDF = np.transpose(TDM.sum(axis=1))\n",
    "IDF = np.array( [np.log(N_D/tf) for tf in IDF] )[0][0] # trick to get rid of the (1,N_V) dimension thing\n",
    "TF_IDF = np.transpose(TDM)*np.diag(IDF)\n",
    "\n",
    "filetosave = 'TFIDF_'+str(N_D)+'docs.npy'\n",
    "np.save(filetosave,TF_IDF)\n",
    "#TF_IDF = csr_matrix(TF_IDF)\n",
    "#save_npz(filetosave,TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distances \n",
    "Distances = np.zeros((N_D,N_D))\n",
    "for i in range(N_D):\n",
    "    for j in range(i+1,N_D):\n",
    "        # we use the standard euclidean distance == norm-2 for vectors\n",
    "        Distances[i,j] = np.linalg.norm( csr_matrix.todense(TDM[:,i] - TDM[:,j]),2 ) #GetDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_features: 4893\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=10, n_init=1, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=10, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 1.078.\n",
      "Mean Inter-Cluster distance = 15.525.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % TF_IDF.shape)\n",
    "km = KMeans(n_clusters=N_K, init='k-means++', max_iter=100, n_init=1, random_state=10)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(TF_IDF)\n",
    "\n",
    "# create the clusters\n",
    "ClustersKM = { i:[] for i in range(N_K)}\n",
    "LabelsKM = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    ClustersKM[LabelsKM[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersKM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_features: 50\n",
      "Clustering sparse data with KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
      "    n_clusters=10, n_init=1, n_jobs=None, precompute_distances='auto',\n",
      "    random_state=10, tol=0.0001, verbose=0)\n",
      "Mean Within-Cluster distance = 0.562.\n",
      "Mean Inter-Cluster distance = 18.173.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Dimensionality reduction\n",
    "svd = TruncatedSVD(50)\n",
    "X = svd.fit_transform(TF_IDF)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "km = KMeans(n_clusters=N_K, init='k-means++', max_iter=100, n_init=1, random_state=10)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "km.fit(X)\n",
    "\n",
    "# create the clusters\n",
    "ClustersKM = { i:[] for i in range(N_K)}\n",
    "LabelsKM = list(km.labels_)\n",
    "for i in range(len(DocList)):\n",
    "    ClustersKM[LabelsKM[i]].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersKM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 100, n_features: 4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/dtchome/kalantzisg/.local/lib/python3.4/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='batch', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=100, mean_change_tol=0.001,\n",
      "             n_components=10, n_jobs=None, n_topics=10, perp_tol=0.1,\n",
      "             random_state=20, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n",
      "Mean Within-Cluster distance = 5.966.\n",
      "Mean Inter-Cluster distance = 7.173.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# we need the raw texts as input for this method\n",
    "FolderToParse = \"BoW_100random/\"#\"Data_Part/\" #pagecontent_as_text/\"\n",
    "RawTexts = []; N=0\n",
    "Subset=100\n",
    "N_Topics=10\n",
    "for document in os.listdir( FolderToParse ):\n",
    "    # load document\n",
    "    FileToLoad = FolderToParse + document\n",
    "    # Load text file of wikipedia entry on bird species\n",
    "    f = open(FileToLoad,'rb')\n",
    "    # we ignore non-printable (strange) letters and symbols !\n",
    "    text = f.read().decode('ascii', 'ignore')\n",
    "    f.close()\n",
    "    RawTexts.append(text)\n",
    "RawTexts = RawTexts[:Subset] # just in case we need a subset of the dataset\n",
    "    \n",
    "vectorizer = TfidfVectorizer(max_df=0.3, max_features=60000, min_df=1, stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(RawTexts)\n",
    "\n",
    "#X = TF_IDF\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "LDA_SKL = LatentDirichletAllocation(n_topics = N_Topics, max_iter=100, random_state=20)\n",
    "LDA_SKL.fit(X)\n",
    "print(\"Clustering sparse data with %s\" % LDA_SKL)\n",
    "# get doc-topic distributions\n",
    "LDA_SKLvectors = LDA_SKL.transform(X)\n",
    "\n",
    "# Clustering - Black and white approach, as before\n",
    "ClustersSKL = { i:[] for i in range(N_Topics)}\n",
    "LabelsSKL = []\n",
    "for i in range(len(DocList)):\n",
    "    distr = list(LDA_SKLvectors[i])\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = distr.index(max(distr))\n",
    "    ClustersSKL[label].append(i)\n",
    "    LabelsSKL.append( label )\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersSKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Within-Cluster distance = 6.175.\n",
      "Mean Inter-Cluster distance = 6.522.\n"
     ]
    }
   ],
   "source": [
    "ClustersRAND = { i:[] for i in range(N_K)}# initialize the dictictionary of clusters\n",
    "for i in range(len(DocList)):\n",
    "    #distr = [ x[1] for x in Pi[i]]\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = np.random.randint(N_K)\n",
    "    ClustersRAND[label].append(i)\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersRAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "#from scipy.sparse import save_npz\n",
    "np.save('100New_Doc_Distances.npy',Distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-7507702bbefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mClusterDist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_K\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m#first we compute the intra-cluster distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Clusters' is not defined"
     ]
    }
   ],
   "source": [
    "# Now compare the distances for different clusters....\n",
    "ClusterDist = np.zeros( (N_K,N_K))\n",
    "for c1 in range(N_K):\n",
    "    if len(Clusters[c1])>0:\n",
    "        for c2 in range(c1,N_K):\n",
    "            #first we compute the intra-cluster distance\n",
    "            if len(Clusters[c2])>0:\n",
    "                ClusterDist[c1,c2] = ComputeDistances(Distances, list(Clusters[c1]), list(Clusters[c2]))\n",
    "\n",
    "def ComputeDistances(D, C1, C2):\n",
    "    \"\"\"\n",
    "        Function that, given two sets of indices C1, C2 and a matrix D with  \n",
    "        distances calculated for every pair, it computes the average distance.\n",
    "    \"\"\"\n",
    "    S=0\n",
    "    for i in range(len(C1)):\n",
    "        for j in range(len(C2)):\n",
    "            S += S + D[ C1[i], C2[j] ]\n",
    "    return S/(len(C1)*len(C2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM = np.load(\"Term_Doc_Matrix_All.npy\")\n",
    "type(TDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<62959x10769 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1301308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Term-Doc matrix is 0.19% dense.\n"
     ]
    }
   ],
   "source": [
    "print(\"The Term-Doc matrix is {0:.2f}% dense.\".format(1301308/(62959*10769)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('Doc_Doc_Distances_All.npz',Distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White-rimmed brush finch\n",
      "White-throated jacamar\n",
      "Willard's sooty boubou\n",
      "Yellow-bellied tit\n",
      "Yellow-eyed starling\n",
      "Black-faced dacnis\n",
      "Black-crowned tchagra\n",
      "Ahanta francolin\n",
      "Black-hooded sierra finch\n",
      "Black-tipped cotinga\n",
      "Blue-black kingfisher\n",
      "Blue-winged teal\n",
      "Brasília tapaculo\n",
      "Brown sicklebill\n",
      "Brush cuckoo\n",
      "Bush blackcap\n",
      "Abbott's babbler\n",
      "Caquetá seedeater\n",
      "Cherry-throated tanager\n",
      "Andaman masked owl\n",
      "Chestnut-hooded laughingthrush\n",
      "Chubut steamer duck\n",
      "Cocoi heron\n",
      "Common ringed plover\n",
      "Crested barbet\n",
      "Cuban gnatcatcher\n",
      "Dieffenbach's rail\n",
      "Dusky-faced tanager\n",
      "Elegant crested tinamou\n",
      "European stonechat\n",
      "Arctic tern\n",
      "Flores green pigeon\n",
      "Franklin's gull\n",
      "Gillett's lark\n",
      "Golden-naped woodpecker\n",
      "Great parrotbill\n",
      "Green racket-tail\n",
      "Grey heron\n",
      "Grey-collared oriole\n",
      "Grey-throated tit-flycatcher\n",
      "Harwood's francolin\n",
      "Australasian gannet\n",
      "Holub's golden weaver\n",
      "Ihering's antwren\n",
      "Jacobin cuckoo\n",
      "Kadavu fantail\n",
      "Labrador duck\n",
      "Lemon-bellied white-eye\n",
      "Line-fronted canastero\n",
      "Long-billed white-eye\n",
      "MacGregor's honeyeater\n",
      "Malayan banded pitta\n",
      "Banded bay cuckoo\n",
      "Marquesan ground dove\n",
      "Melodious babbler\n",
      "Montane nightjar\n",
      "Moustached barbet\n",
      "New Caledonian thicketbird\n",
      "Hen harrier\n",
      "Okarito kiwi\n",
      "Orange-breasted laughingthrush\n",
      "Paddyfield pipit\n",
      "Swallow-tailed cotinga\n",
      "Barred owlet-nightjar\n",
      "Pearly-breasted cuckoo\n",
      "Pied honeyeater\n",
      "Plain-brown woodcreeper\n",
      "Puerto Rican oriole\n",
      "Radde's accentor\n",
      "Red-breasted meadowlark\n",
      "Red-headed parrotfinch\n",
      "Red-winged parrot\n",
      "Rockefeller's sunbird\n",
      "Rufous babbler\n",
      "Grey-banded babbler\n",
      "Rufous-crowned antpitta\n",
      "Rufous-webbed bush tyrant\n",
      "Saddle-billed stork\n",
      "Satin bowerbird\n",
      "Scissor-tailed kite\n",
      "Shining-blue kingfisher\n",
      "Silvery pigeon\n",
      "Small niltava\n",
      "Sooty barbet\n",
      "Spangle-cheeked tanager\n",
      "Black turnstone\n",
      "Spot-throated woodcreeper\n",
      "Saint Lucia black finch\n",
      "Striated babbler\n",
      "Sulawesi scops owl\n",
      "Swee waxbill\n",
      "Tawny-breasted tinamou\n",
      "Ticking doradito\n",
      "Tucumán mountain finch\n",
      "Variable wheatear\n",
      "Violet-throated starfrontlet\n",
      "Black-breasted buttonquail\n",
      "Western capercaillie\n",
      "White-bellied chachalaca\n",
      "White-browed spinetail\n"
     ]
    }
   ],
   "source": [
    "with open('list_of_species.txt', encoding='utf-8', errors='ignore') as f:\n",
    "    Names = f.readlines()\n",
    "Names = [x.strip() for x in Names]\n",
    "Names_100New = []\n",
    "for document in os.listdir( \"BoW_100random/\" ):\n",
    "    temp = Names[ int(document[:-8])-1 ]\n",
    "    #print( temp )\n",
    "    Names_100New.append( temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# we need the raw texts as input for this method\n",
    "FolderToParse = \"BoW_100random/\"#\"Data_Part/\" #pagecontent_as_text/\"\n",
    "RawTexts = []; N=0\n",
    "for document in os.listdir( FolderToParse ):\n",
    "    # load document\n",
    "    FileToLoad = FolderToParse + document\n",
    "    # Load text file of wikipedia entry on bird species\n",
    "    f = open(FileToLoad,'rb')\n",
    "    # we ignore non-printable (strange) letters and symbols !\n",
    "    text = f.read().decode('ascii', 'ignore')\n",
    "    f.close()\n",
    "    RawTexts.append(text)\n",
    "RawTexts = RawTexts[:Subset] # just in case we need a subset of the dataset\n",
    "    \n",
    "vectorizer = TfidfVectorizer(max_df=0.3, max_features=60000, min_df=1, stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(RawTexts)\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "LDA_SKL = LatentDirichletAllocation(n_topics = N_Topics, max_iter=100, random_state=20)\n",
    "LDA_SKL.fit(X)\n",
    "print(\"Clustering sparse data with %s\" % LDA_SKL)\n",
    "# get doc-topic distributions\n",
    "LDA_SKLvectors = LDA_SKL.transform(X)\n",
    "\n",
    "# Clustering - Black and white approach, as before\n",
    "ClustersSKL = { i:[] for i in range(N_Topics)}\n",
    "LabelsSKL = []\n",
    "for i in range(len(DocList)):\n",
    "    distr = list(LDA_SKLvectors[i])\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = distr.index(max(distr))\n",
    "    ClustersSKL[label].append(i)\n",
    "    LabelsSKL.append( label )\n",
    "    \n",
    "# Evaluate\n",
    "t = EvalClustering(Distances, ClustersSKL)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
