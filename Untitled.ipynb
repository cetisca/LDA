{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10769 documents and 62959 words in total!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "#from sklearn.metrics.pairwise import pairwise_distances\n",
    "import os\n",
    "\n",
    "FolderToParse = \"BagsOfWords/\"\n",
    "DocList = []\n",
    "for document in os.listdir( FolderToParse ):\n",
    "    # load documents\n",
    "    FileToLoad = FolderToParse + document\n",
    "    f = open(FileToLoad,'rb')\n",
    "    words = f.read().decode('ascii', 'ignore')\n",
    "    f.close()\n",
    "    words = words.split()\n",
    "    DocList.append(words)\n",
    "X_words = DocList#[:100]\n",
    "\n",
    "corpus=np.unique(np.concatenate(X_words))\n",
    "N_D = len(X_words) # number of documents\n",
    "N_V = len(corpus)  # number of vocabulary - all available words\n",
    "N_K = 10 # set the number of topics\n",
    "print(\"We have {0} documents and {1} words in total!\".format(N_D, N_V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 500 documents have been processed! Rate = 0.10709242782516326\n",
      "More than 1000 documents have been processed! Rate = 0.10519153530055934\n",
      "More than 1500 documents have been processed! Rate = 0.1092377464480206\n",
      "More than 2000 documents have been processed! Rate = 0.11986428573764879\n",
      "More than 2500 documents have been processed! Rate = 0.12131917853506148\n",
      "More than 3000 documents have been processed! Rate = 0.12405843629802056\n",
      "More than 3500 documents have been processed! Rate = 0.13286942493987103\n",
      "More than 4000 documents have been processed! Rate = 0.13615808775497096\n",
      "More than 4500 documents have been processed! Rate = 0.13429439096987103\n",
      "More than 5000 documents have been processed! Rate = 0.13676846191915995\n",
      "More than 5500 documents have been processed! Rate = 0.13730150293189627\n",
      "More than 6000 documents have been processed! Rate = 0.13681055792452276\n",
      "More than 6500 documents have been processed! Rate = 0.13575175905763268\n",
      "More than 7000 documents have been processed! Rate = 0.133901112930215\n",
      "More than 7500 documents have been processed! Rate = 0.13454923408478797\n",
      "More than 8000 documents have been processed! Rate = 0.13569217286179672\n",
      "More than 8500 documents have been processed! Rate = 0.1328937028040394\n",
      "More than 9000 documents have been processed! Rate = 0.13180329058087922\n",
      "More than 9500 documents have been processed! Rate = 0.1311486569790228\n",
      "More than 10000 documents have been processed! Rate = 0.12937360404074008\n",
      "More than 10500 documents have been processed! Rate = 0.1297087058281601\n",
      "The Term-Doc matrix is 0.19% dense.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# initialize term - document matrix\n",
    "#TDM = csr_matrix((N_V,N_D))\n",
    "TDM = np.zeros((N_V,N_D)) # we start by a full matrix and then transform it\n",
    "#TDM = [] \n",
    "t0 = time()\n",
    "# create doc-word list of lists\n",
    "for doc in range(N_D):\n",
    "    temp = np.unique( X_words[doc] ) # get the different words on this document\n",
    "    #temp2 = np.zeros((N_V,1))\n",
    "    for i in range( len(temp) ):\n",
    "        word = temp[i] \n",
    "        count = len([ x for x in X_words[doc] if x == word])\n",
    "        # we must get the index of this word in the (total) corpus\n",
    "        TDM[ np.where(corpus == word) , doc] = count\n",
    "        #temp2[ np.where(corpus == word) ] = count\n",
    "    #TDM.append(temp2)\n",
    "    # progress check\n",
    "    if ((doc+1) % 500) == 0 :\n",
    "        print('More than {0} documents have been processed! Rate = {1}'.format(doc+1, (time()-t0)/doc))\n",
    "\n",
    "# sanity check\n",
    "for doc in range(N_D):\n",
    "    if sum(TDM[:,doc])!=len(X_words[doc]):\n",
    "        print(\"Doc-{0} has a problem!\".format(doc))\n",
    "TDM = csr_matrix(TDM)\n",
    "print(\"The Term-Doc matrix is {0:.2f}% dense.\".format(csr_matrix.count_nonzero(TDM)/np.prod(TDM.shape)*100))\n",
    "#print(\"The Term-Doc matrix is {0:.2f}% dense.\".format(np.count_nonzero(TDM)/np.prod(TDM.shape)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE IT\n",
    "import scipy.sparse\n",
    "#scipy.sparse.save_npz('Term_Doc_Matrix_All.npz',TDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Black and white clustering - the number of clusters is N_K\n",
    "# initialize the dictictionary of clusters\n",
    "Clusters = { i:[] for i in range(N_K)}\n",
    "ClustersNames = { i:[] for i in range(N_K)} \n",
    "Labels = []\n",
    "for i in range(len(DocList)):\n",
    "    distr = [ x[1] for x in Pi[i]]\n",
    "    # find the argmax{distr} - ATTENTION: ties ???\n",
    "    label = distr.index(max(distr))\n",
    "    Clusters[label].append(i)\n",
    "    ClustersNames[label].append(Names[i])\n",
    "    Labels.append( label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate distances \n",
    "Distances = np.zeros((N_D,N_D))\n",
    "for i in range(N_D):\n",
    "    for j in range(i+1,N_D):\n",
    "        # we use the standard euclidean distance == norm-2 for vectors\n",
    "        Distances[i,j] = np.linalg.norm( csr_matrix.todense(TDM[:,i] - TDM[:,j]),2 ) #GetDistance\n",
    "# Now compare the distances for different clusters....\n",
    "ClusterDist = np.zeros( (N_K,N_K))\n",
    "for c1 in range(N_K):\n",
    "    if len(Clusters[c1])>0:\n",
    "        for c2 in range(c1,N_K):\n",
    "            #first we compute the intra-cluster distance\n",
    "            if len(Clusters[c2])>0:\n",
    "                ClusterDist[c1,c2] = ComputeDistances(Distances, list(Clusters[c1]), list(Clusters[c2]))\n",
    "\n",
    "def ComputeDistances(D, C1, C2):\n",
    "    \"\"\"\n",
    "        Function that, given two sets of indices C1, C2 and a matrix D with  \n",
    "        distances calculated for every pair, it computes the average distance.\n",
    "    \"\"\"\n",
    "    S=0\n",
    "    for i in range(len(C1)):\n",
    "        for j in range(len(C2)):\n",
    "            S += S + D[ C1[i], C2[j] ]\n",
    "    return S/(len(C1)*len(C2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('Doc_Doc_Distances_All.npz',Distances)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda 3]",
   "language": "python",
   "name": "Python [Anaconda 3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
