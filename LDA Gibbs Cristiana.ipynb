{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA using Gibbs Sampling\n",
    "###  Formulae from Kevin Murphy: Machine Learning - A Probabilistic Perspective, Chapter 27 *[see updated latex doc ]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import each document as a list of words (output of preprocessing algorithm). Input this into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_words = np.array([\n",
    "    ['abdim', 'stork', 'ciconia', 'abdimii', 'also', 'known', 'white', 'bellied', 'stork', 'black', 'stork', 'grey', 'leg', 'red', 'knee', 'foot', 'grey', 'bill', 'white', 'underpart', 'red', 'facial', 'skin', 'front', 'eye', 'blue', 'skin', 'near', 'bill', 'breeding', 'season', 'smallest', 'specie', 'stork', 'weight', 'lb', 'female', 'lay', 'two', 'three', 'egg', 'slightly', 'smaller', 'male', 'abdim', 'stork', 'distributed', 'open', 'habitat', 'throughout', 'eastern', 'africa', 'ethiopia', 'south', 'south', 'africa', 'diet', 'consists', 'mainly', 'locust', 'caterpillar', 'large', 'insect', 'although', 'bird', 'also', 'eat', 'small', 'reptile', 'amphibian', 'mouse', 'crab', 'egg', 'among', 'smallest', 'stork', 'specie', 'welcomed', 'protected', 'local', 'african', 'belief', 'harbinger', 'rain', 'good', 'luck', 'name', 'commemorates', 'turkish', 'governor', 'wadi', 'halfa', 'sudan', 'bey', 'arnaut', 'abdim', 'widespread', 'common', 'throughout', 'large', 'range', 'abdim', 'stork', 'evaluated', 'least', 'concern', 'iucn', 'red', 'list', 'threatened', 'specie', 'subject', 'several', 'nationally', 'coordinated', 'breeding', 'program', 'united', 'state', 'plan', 'specie', 'administered', 'association', 'zoo', 'aquarium', 'europe', 'european', 'association', 'zoo', 'aquarium', 'san', 'diego', 'zoo', 'egg', 'ciconia', 'abdimii', 'mus√©um', 'toulouse', 'africa', 'distribution', 'abdim', 'stork'], #each row represents an individual document\n",
    "    ['abd', 'kuri', 'sparrow', 'passer', 'hemileucus', 'passerine', 'bird', 'endemic', 'small', 'island', 'abd', 'kuri', 'also', 'spelled', 'several', 'way', 'socotra', 'archiplago', 'indian', 'ocean', 'horn', 'africa', 'though', 'specie', 'originally', 'described', 'distinct', 'specie', 'considered', 'conspecific', 'socotra', 'sparrow', 'study', 'guy', 'kirwan', 'showed', 'significant', 'difference', 'socotra', 'sparrow', 'two', 'sparrow', 'might', 'even', 'different', 'origin', 'evidence', 'morphologically', 'distinct', 'birdlife', 'international', 'hence', 'iucn', 'red', 'list', 'recognised', 'specie', 'listed', 'ioc', 'world', 'bird', 'list', 'december', 'restricted', 'distribution', 'population', 'individual', 'despite', 'known', 'threat', 'considered', 'vulnerable', 'specie', 'iucn', 'red', 'list'],\n",
    "    ['nectarinia', 'sovimanga', 'souimanga', 'sunbird', 'cinnyris', 'sovimanga', 'small', 'passerine', 'bird', 'sunbird', 'family', 'nectariniidae', 'native', 'island', 'western', 'indian', 'ocean', 'occurs', 'madagascar', 'aldabra', 'group', 'glorioso', 'island', 'souimanga', 'sunbird', 'long', 'wingspan', 'black', 'bill', 'long', 'thin', 'curved', 'male', 'nominate', 'subspecies', 'metallic', 'green', 'head', 'back', 'throat', 'breast', 'blackish', 'le', 'continuous', 'red', 'band', 'belly', 'yellow', 'wing', 'tail', 'brown', 'yellow', 'tuft', 'side', 'breast', 'become', 'visible', 'bird', 'lift', 'wing', 'courtship', 'display', 'male', 'presumably', 'moult', 'duller', 'eclipse', 'plumage', 'march', 'april', 'losing', 'metallic', 'red', 'feathering', 'month', 'female', 'grey', 'brown', 'upperparts', 'dull', 'yellow', 'belly', 'grey', 'throat', 'breast', 'darker', 'marking', 'juvenile', 'similar', 'adult', 'female', 'chin', 'throat', 'sometimes', 'black', 'upperparts', 'may', 'olive', 'abbott', 'sunbirds', 'see', 'larger', 'long', 'wingspan', 'male', 'broader', 'red', 'breastband', 'yellow', 'underpart', 'dark', 'brown', 'ssp', 'abbotti', 'blackish', 'ssp', 'buchenorum', 'male', 'bird', 'nominate', 'group', 'wing', 'tail', 'long', 'bill', 'female', 'measure', 'le', 'bird', 'chirruping', 'flight', 'call', 'loud', 'hoarse', 'alarm', 'call', 'male', 'sings', 'fast', 'scratchy', 'song', 'frequently', 'repeated', 'phrase', 'five', 'subspecies', 'nominate', 'subspecies', 'sovimanga', 'found', 'across', 'madagascar', 'glorioso', 'island', 'south', 'western', 'madagascar', 'replaced', 'apolis', 'aldabrensis', 'aldabra', 'atoll', 'abbotti', 'assumption', 'island', 'buchenorum', 'cosmoledo', 'astove', 'form', 'abbotti', 'buchenorum', 'sometimes', 'considered', 'separate', 'specie', 'abbott', 'sunbird', 'would', 'called', 'cinnyris', 'abbotti', 'citation', 'needed', 'sunbird', 'one', 'separated', 'former', 'catch', 'genus', 'nectarinia', 'cinnyris', 'citation', 'needed', 'together', 'malagasy', 'white', 'eye', 'madagascan', 'cisticola', 'souimanga', 'sunbirds', 'common', 'small', 'landbirds', 'across', 'much', 'range', 'ample', 'stock', 'present', 'specie', 'white', 'eye', 'exist', 'maybe', 'square', 'habitat', 'glorioso', 'island', 'iucn', 'considers', 'specie', 'least', 'concern', 'souimanga', 'sunbird', 'found', 'variety', 'habitat', 'mountain', 'forest', 'mangrove', 'scrubland', 'well', 'park', 'garden', 'human', 'modified', 'ecosystem', 'use', 'curved', 'bill', 'probe', 'flower', 'nectar', 'also', 'feed', 'insect', 'spider', 'natural', 'enemy', 'nest', 'inaccessible', 'predator', 'long', 'breeding', 'season', 'last', 'august', 'march', 'aldabra', 'least', 'nest', 'dome', 'shaped', 'entrance', 'hole', 'side', 'made', 'plant', 'material', 'grass', 'stem', 'coconut', 'fibre', 'leaf', 'usually', 'suspended', 'branch', 'metre', 'ground', 'may', 'built', 'building', 'sinkhole', 'within', 'eroded', 'coral', 'two', 'egg', 'laid', 'incubated', 'day', 'whitish', 'reddish', 'mottling', 'young', 'bird', 'fledge', 'day', 'nest', 'building', 'incubation', 'egg', 'done', 'female', 'also', 'play', 'greater', 'role', 'male', 'feeding', 'chick'],\n",
    "    ['abyssian', 'wheatear', 'oenanthe', 'lugubris', 'specie', 'bird', 'family', 'muscicapidae', 'found', 'northwestern', 'africa', 'southern', 'kenya', 'northeastern', 'tanzania'],\n",
    "    ['abyssinian', 'white', 'eye', 'white', 'breasted', 'white', 'eye', 'zosterops', 'abyssinicus', 'small', 'passerine', 'bird', 'belonging', 'genus', 'zosterops', 'white', 'eye', 'family', 'zosteropidae', 'native', 'north', 'east', 'africa', 'southern', 'arabia', 'long', 'upperparts', 'green', 'darker', 'greyer', 'northern', 'race', 'narrow', 'white', 'ring', 'around', 'eye', 'thin', 'black', 'line', 'bill', 'eye', 'underpart', 'vary', 'pale', 'yellow', 'greyish', 'white', 'depending', 'race', 'bird', 'various', 'twittering', 'buzzing', 'call', 'africa', 'occurs', 'north', 'east', 'sudan', 'south', 'eritrea', 'ethiopia', 'northern', 'somalia', 'kenya', 'north', 'east', 'tanzania', 'also', 'found', 'socotra', 'island', 'arabia', 'occurs', 'south', 'west', 'saudi', 'arabia', 'yemen', 'southern', 'oman', 'occurs', 'open', 'woodland', 'scrub', 'wadi', 'garden', 'found', 'metre', 'sea', 'level', 'africa', 'metre', 'arabia', 'usually', 'forage', 'among', 'branch', 'tree', 'sometimes', 'descends', 'ground', 'level', 'feed', 'mainly', 'insect', 'also', 'take', 'nectar', 'flower'],\n",
    "    ['picus', 'abyssinicus', 'abyssinian', 'woodpecker', 'dendropicos', 'abyssinicus', 'also', 'known', 'golden', 'backed', 'woodpecker', 'golden', 'mantled', 'woodpecker', 'specie', 'bird', 'woodpecker', 'family', 'picidae', 'native', 'africa', 'occurs', 'eritrea', 'ethiopia', 'appears', 'close', 'relative', 'cardinal', 'woodpecker', 'dendropicos', 'fuscescens', 'abyssinian', 'woodpecker', 'small', 'woodpecker', 'relatively', 'long', 'broad', 'bill', 'golden', 'yellow', 'back', 'mantle', 'bright', 'red', 'rump', 'barred', 'wing', 'barred', 'tail', 'underpart', 'pale', 'heavily', 'streaked', 'black', 'head', 'striped', 'male', 'distinguished', 'red', 'nape', 'crown', 'brown', 'stripe', 'eye', 'golden', 'mantle', 'separate', 'specie', 'related', 'cardinal', 'woodpecker', 'measured', 'length', 'weighs', '26g', 'abyssinian', 'woodpecker', 'endemic', 'ethiopian', 'highland', 'central', 'eritrea', 'east', 'harar', 'ethiopia', 'river', 'alata', 'tributary', 'hanger', 'river', 'specie', 'occurs', 'juniper', 'wood', 'hagenia', 'forest', 'also', 'area', 'euphorbia', 'particularly', '1600m', '3000m', 'occasionally', 'higher', 'also', 'found', 'wooded', 'savanna', 'lower', 'altitude', 'biology', 'ecology', 'abyssinian', 'woodpecker', 'almost', 'unknown', 'unobtrusive', 'bird', 'probe', 'food', 'among', 'moss', 'growing', 'tree', 'thought', 'nesting', 'period', 'probably', 'run', 'december', 'may', 'abyssinian', 'woodpecker', 'large', 'range', 'hence', 'approach', 'threshold', 'vulnerable', 'currently', 'classed', 'least', 'concern', 'thought', 'decreasing', 'population', 'contracting', 'range', 'due', 'continuing', 'clearance', 'woodland']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Create a dictionary/corpus of all the words in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus/vocabulary\n",
    "corpus=np.unique(np.concatenate(X_words),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Each word needs to be represented by a number (here it is the entry in the dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number matrix (Replacing words in documents with word IDs)\n",
    "X_number = np.copy(X_words)\n",
    "for doc_number in range(X_number.shape[0]):\n",
    "    for doc_length in range(len(X_number[doc_number])):\n",
    "        X_number[doc_number][doc_length]=  np.where(corpus==X_number[doc_number][doc_length])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([6, 387, 81, 7, 21, 222, 439, 48, 387, 56, 387, 184, 234, 332, 221, 166, 184, 52, 439, 418, 332, 153, 365, 173, 152, 58, 365, 285, 52, 63, 353, 369, 379, 387, 433, 230, 159, 229, 417, 406, 137, 366, 368, 254, 6, 387, 124, 304, 191, 409, 132, 14, 143, 375, 375, 14, 118, 94, 252, 242, 76, 226, 212, 22, 54, 21, 133, 367, 339, 25, 275, 103, 137, 24, 369, 387, 379, 434, 327, 241, 15, 47, 196, 329, 179, 247, 279, 89, 415, 180, 430, 193, 394, 51, 35, 6, 441, 90, 409, 226, 330, 6, 387, 148, 233, 91, 216, 332, 239, 405, 379, 392, 357, 282, 99, 63, 326, 419, 384, 316, 379, 12, 37, 454, 31, 146, 147, 37, 454, 31, 346, 117, 454, 137, 81, 7, 278, 411, 14, 125, 6, 387]),\n",
       "       list([5, 223, 378, 310, 199, 311, 54, 138, 367, 215, 5, 223, 21, 380, 357, 431, 370, 33, 210, 299, 205, 14, 402, 379, 306, 115, 122, 379, 92, 95, 370, 378, 391, 190, 220, 359, 361, 119, 370, 378, 417, 378, 267, 149, 120, 305, 150, 270, 122, 55, 213, 200, 216, 332, 239, 331, 379, 240, 214, 449, 54, 239, 110, 340, 125, 320, 211, 116, 222, 404, 92, 429, 379, 216, 332, 239]),\n",
       "       list([287, 377, 374, 395, 82, 377, 367, 311, 54, 395, 154, 288, 283, 215, 437, 210, 299, 298, 250, 18, 188, 177, 215, 374, 395, 243, 443, 56, 52, 243, 401, 106, 254, 292, 393, 265, 183, 197, 42, 408, 60, 57, 231, 97, 332, 44, 49, 451, 442, 398, 67, 451, 414, 360, 60, 46, 428, 54, 237, 442, 102, 121, 254, 323, 273, 130, 134, 319, 258, 30, 244, 265, 332, 156, 269, 159, 184, 67, 422, 129, 451, 49, 184, 408, 60, 108, 259, 218, 362, 13, 159, 79, 408, 372, 56, 422, 261, 301, 3, 396, 354, 227, 243, 443, 254, 66, 332, 61, 451, 418, 107, 67, 383, 4, 57, 383, 68, 254, 54, 292, 188, 442, 398, 243, 52, 159, 263, 231, 54, 80, 163, 72, 245, 203, 16, 72, 254, 363, 155, 349, 373, 172, 337, 313, 161, 393, 292, 393, 377, 171, 11, 250, 177, 215, 375, 437, 250, 338, 27, 19, 18, 40, 4, 38, 215, 68, 101, 39, 169, 4, 68, 372, 92, 355, 379, 3, 395, 450, 73, 82, 4, 84, 289, 395, 303, 356, 170, 75, 176, 287, 82, 84, 289, 410, 253, 439, 152, 249, 83, 374, 396, 90, 367, 225, 11, 276, 330, 26, 386, 322, 379, 439, 152, 151, 262, 382, 191, 177, 215, 216, 93, 379, 233, 91, 374, 395, 171, 425, 191, 274, 168, 255, 351, 435, 308, 175, 206, 268, 136, 423, 106, 52, 325, 164, 286, 21, 157, 212, 381, 284, 139, 290, 207, 321, 243, 63, 353, 228, 41, 258, 18, 233, 290, 126, 358, 140, 204, 360, 251, 317, 260, 181, 385, 88, 160, 232, 424, 397, 59, 266, 187, 261, 70, 69, 364, 444, 142, 100, 417, 137, 224, 208, 109, 440, 333, 272, 453, 54, 162, 109, 290, 69, 209, 137, 127, 159, 21, 318, 182, 343, 254, 158, 78]),\n",
       "       list([8, 438, 300, 248, 379, 54, 154, 277, 171, 296, 14, 376, 219, 294, 400]),\n",
       "       list([9, 439, 152, 439, 62, 439, 152, 456, 10, 367, 311, 54, 50, 176, 456, 439, 152, 154, 455, 283, 293, 131, 14, 376, 32, 243, 422, 183, 108, 185, 295, 328, 281, 439, 341, 36, 152, 401, 56, 238, 52, 152, 418, 427, 307, 451, 186, 439, 113, 328, 54, 426, 416, 71, 72, 14, 298, 293, 131, 394, 375, 141, 143, 295, 371, 219, 293, 131, 400, 21, 171, 370, 215, 32, 298, 375, 436, 347, 32, 452, 376, 302, 298, 304, 447, 350, 430, 175, 171, 266, 352, 236, 14, 266, 32, 424, 167, 24, 59, 412, 372, 114, 187, 236, 157, 252, 212, 21, 399, 286, 164]),\n",
       "       list([315, 10, 9, 448, 112, 10, 21, 222, 178, 43, 448, 178, 257, 448, 379, 54, 448, 154, 314, 283, 14, 298, 141, 143, 28, 87, 335, 74, 448, 112, 174, 9, 448, 367, 448, 336, 243, 65, 52, 178, 451, 42, 256, 64, 332, 344, 45, 442, 45, 398, 418, 307, 198, 388, 56, 197, 390, 254, 123, 332, 280, 104, 67, 389, 152, 178, 256, 355, 379, 334, 74, 448, 264, 235, 432, 1, 9, 448, 138, 144, 202, 77, 141, 131, 195, 143, 342, 17, 413, 194, 342, 379, 298, 217, 445, 192, 168, 21, 34, 145, 309, 0, 2, 297, 201, 21, 171, 446, 348, 246, 23, 53, 135, 9, 448, 20, 420, 421, 54, 325, 165, 24, 271, 189, 412, 403, 291, 312, 324, 345, 110, 261, 9, 448, 226, 330, 200, 29, 407, 429, 105, 85, 233, 91, 403, 111, 320, 98, 330, 128, 96, 86, 447])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4**: Initialise variables (number of documents, chosen number of topics, Dirichlet priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_D = X_words.shape[0] #number of documents\n",
    "N_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_K = 5 # num of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_W = corpus.shape[0] # words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dirichlet priors\n",
    "alpha = 1 # Choice of alpha affects document clustering \n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Initialise the word topic assignment matrix (ziv in Murphy Chapter), document topic distribution (pi in chapter) and word topic distribution (betak in chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Z := word topic assignment\n",
    "Z = np.copy(X_number)\n",
    "\n",
    "for doc_number in range(Z.shape[0]):\n",
    "    for doc_length in range(len(Z[doc_number])):\n",
    "        Z[doc_number][doc_length]= np.random.randint(N_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 2, 2, 1, 4, 1, 4, 3, 3, 0, 4, 1, 2, 0, 1, 4, 1, 4, 0, 2, 3, 1, 0, 2, 1, 1, 4, 2, 4, 0, 1, 4, 2, 1, 1, 2, 3, 1, 0, 1, 1, 3, 4, 4, 1, 0, 1, 4, 3, 3, 0, 4, 1, 4, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 1, 2, 4, 3, 2, 0, 2, 1, 3, 2, 3, 3, 0, 4, 0, 3, 4, 1, 4, 1, 4, 0, 4, 3, 1, 4, 2, 4, 3, 3, 0, 3, 0, 2, 0, 1, 0, 1, 3, 3, 2, 2, 0, 1, 1, 3, 2, 4, 2, 2, 4, 0, 2, 3, 2, 1, 1, 0, 4, 1, 2, 3, 0, 2, 4, 4, 0, 3, 1, 3, 2, 3, 1, 2, 1, 0, 3, 2]),\n",
       "       list([2, 1, 0, 3, 1, 1, 2, 2, 0, 3, 3, 1, 4, 1, 0, 1, 2, 0, 0, 0, 2, 0, 4, 2, 1, 3, 3, 1, 1, 4, 3, 2, 4, 1, 2, 2, 4, 3, 2, 4, 3, 4, 0, 3, 4, 0, 2, 1, 1, 1, 0, 0, 2, 3, 0, 0, 0, 1, 1, 4, 1, 2, 3, 1, 2, 3, 2, 2, 1, 3, 1, 3, 2, 1, 3, 0]),\n",
       "       list([0, 4, 4, 1, 1, 0, 4, 0, 2, 2, 4, 0, 2, 1, 4, 2, 1, 1, 4, 4, 0, 2, 3, 4, 1, 3, 4, 1, 3, 3, 4, 3, 2, 1, 4, 4, 2, 2, 0, 4, 2, 4, 0, 3, 3, 4, 0, 3, 2, 3, 0, 4, 2, 2, 1, 0, 3, 4, 4, 4, 2, 2, 1, 1, 1, 2, 2, 3, 0, 4, 2, 1, 4, 4, 0, 1, 2, 4, 4, 2, 3, 0, 2, 3, 0, 2, 0, 2, 0, 4, 2, 4, 4, 3, 4, 0, 3, 1, 2, 0, 4, 3, 0, 2, 4, 1, 2, 2, 4, 0, 1, 3, 4, 0, 4, 1, 0, 0, 0, 1, 0, 0, 4, 1, 2, 3, 4, 3, 0, 2, 2, 0, 3, 4, 1, 2, 2, 1, 1, 1, 4, 0, 3, 2, 3, 0, 3, 3, 3, 2, 1, 4, 3, 3, 2, 3, 2, 4, 1, 0, 1, 3, 0, 0, 1, 3, 1, 3, 0, 2, 3, 3, 4, 4, 2, 2, 3, 4, 3, 1, 0, 4, 4, 0, 3, 1, 2, 0, 1, 3, 0, 0, 4, 1, 0, 0, 4, 2, 3, 2, 2, 0, 1, 0, 4, 0, 1, 3, 2, 0, 4, 2, 3, 1, 0, 0, 4, 4, 3, 3, 2, 3, 1, 1, 4, 3, 4, 2, 3, 0, 2, 1, 3, 3, 0, 3, 0, 2, 4, 4, 4, 4, 3, 3, 0, 1, 2, 4, 3, 1, 2, 2, 0, 3, 2, 2, 1, 3, 3, 1, 4, 0, 0, 2, 2, 2, 4, 1, 1, 3, 1, 0, 3, 2, 2, 3, 3, 0, 1, 3, 2, 4, 3, 1, 4, 2, 3, 1, 3, 2, 0, 0, 2, 1, 4, 0, 0, 4, 0, 0, 4, 3, 3, 4, 1, 4, 3, 4, 1, 1, 2, 4, 1]),\n",
       "       list([0, 2, 2, 2, 4, 4, 4, 3, 3, 3, 4, 3, 4, 1, 4]),\n",
       "       list([3, 4, 3, 4, 0, 1, 0, 0, 4, 3, 3, 3, 4, 3, 2, 3, 0, 2, 3, 4, 1, 4, 0, 0, 1, 0, 0, 1, 1, 4, 3, 3, 4, 2, 4, 3, 0, 1, 0, 3, 2, 1, 3, 0, 1, 2, 4, 0, 1, 3, 0, 1, 0, 3, 4, 3, 3, 2, 1, 0, 0, 3, 4, 4, 0, 0, 2, 1, 3, 3, 3, 0, 3, 2, 3, 1, 3, 4, 2, 0, 3, 3, 3, 3, 4, 4, 4, 3, 2, 3, 0, 4, 4, 0, 2, 3, 3, 2, 1, 1, 0, 0, 0, 4, 3, 1, 0, 4, 2, 2, 3]),\n",
       "       list([1, 1, 2, 3, 3, 2, 3, 4, 0, 2, 3, 2, 0, 1, 3, 4, 0, 1, 2, 0, 4, 4, 1, 3, 4, 3, 4, 3, 4, 4, 1, 1, 2, 0, 4, 3, 1, 3, 0, 3, 0, 3, 4, 3, 0, 0, 2, 1, 1, 1, 1, 3, 0, 1, 3, 1, 4, 4, 0, 2, 0, 1, 1, 4, 4, 3, 4, 4, 0, 3, 2, 2, 1, 2, 0, 3, 1, 3, 2, 4, 4, 1, 2, 1, 1, 4, 0, 0, 3, 1, 3, 0, 1, 1, 0, 3, 0, 0, 1, 0, 2, 4, 0, 4, 3, 1, 3, 2, 4, 4, 4, 1, 1, 0, 0, 3, 2, 1, 2, 0, 3, 2, 4, 1, 4, 4, 1, 4, 3, 0, 2, 4, 2, 0, 1, 0, 1, 4, 3, 0, 1, 4, 3, 2, 3, 2, 3, 0, 1, 2, 0, 0, 0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pi := document topic distribution\n",
    "Pi = np.zeros([N_D, N_K])\n",
    "\n",
    "for i in range(N_D):\n",
    "    Pi[i] = np.random.dirichlet(alpha*np.ones(N_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51694088, 0.09386642, 0.05062893, 0.19864925, 0.13991452],\n",
       "       [0.14574999, 0.35852712, 0.04693749, 0.04239945, 0.40638594],\n",
       "       [0.3222704 , 0.27821982, 0.27127213, 0.06879677, 0.05944088],\n",
       "       [0.11220868, 0.0418284 , 0.57862944, 0.23329878, 0.0340347 ],\n",
       "       [0.24021097, 0.14794737, 0.31603626, 0.117709  , 0.17809639],\n",
       "       [0.42607053, 0.36385843, 0.03081259, 0.02280333, 0.15645512]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B := word topic distribution\n",
    "B = np.zeros([N_K, N_W])\n",
    "\n",
    "for k in range(N_K):\n",
    "    B[k] = np.random.dirichlet(gamma*np.ones(N_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.49576881e-03, 1.09645404e-03, 1.22467537e-03, ...,\n",
       "        2.35469654e-03, 4.25232152e-03, 7.03932381e-03],\n",
       "       [1.26057801e-03, 3.71792569e-04, 2.30764950e-03, ...,\n",
       "        1.87141265e-03, 1.47361418e-03, 4.88317346e-04],\n",
       "       [1.18017974e-03, 6.36964954e-04, 3.85567781e-03, ...,\n",
       "        1.95773246e-03, 3.20198082e-04, 4.27380075e-03],\n",
       "       [1.48827488e-02, 6.09928851e-04, 1.63317724e-03, ...,\n",
       "        2.94066195e-05, 2.63832672e-03, 1.93565240e-04],\n",
       "       [2.27533369e-03, 1.35511511e-03, 2.83876418e-03, ...,\n",
       "        2.73967226e-04, 1.09300826e-03, 4.78210872e-05]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 457)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: ** Use Gibbs sampling to update parameters. Need many iterations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for iterations in range(10):  #Need at least 1000 iterations for Gibbs sampling to work!\n",
    "    \n",
    "    \n",
    "    # Updating Z matrix\n",
    "    for doc_number in range(Z.shape[0]):     \n",
    "        for doc_length in range(len(Z[doc_number])):\n",
    "            \n",
    "             # Calculate params for Z\n",
    "            p_iv = np.exp(np.log(Pi[i]) + np.log(B[:, X_number[doc_number][ doc_length]]))\n",
    "            p_iv /= np.sum(p_iv)\n",
    "\n",
    "             # Resample word topic assignment Z\n",
    "            Z[doc_number][doc_length] = np.random.multinomial(1, p_iv).argmax()\n",
    "     \n",
    "    # Updating Pi   \n",
    "    for i in range(N_D):\n",
    "        m = np.zeros(N_K)\n",
    "\n",
    "        # Gather sufficient statistics\n",
    "        for k in range(N_K):\n",
    "            m[k] = np.sum(Z[i] == k)\n",
    "\n",
    "        # Resample doc topic dist.\n",
    "        Pi[i, :] = np.random.dirichlet(alpha + m)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    #Updating B\n",
    "    \n",
    "    for k in range(N_K):\n",
    "        print(k)\n",
    "        n = np.zeros(N_W)\n",
    "    \n",
    "        #Gather statistics\n",
    "        \n",
    "        for v in range(N_W):\n",
    "            for doc_number in range(Z.shape[0]):\n",
    "                for doc_length in range(len(Z[doc_number])):\n",
    "                    n[v] += (X_number[doc_number][ doc_length]==v) and (Z[doc_number][doc_length] ==k)\n",
    "        \n",
    "        # Resample word topic distribution\n",
    "        \n",
    "        B[k,:] = np.random.dirichlet(gamma+n)\n",
    "        \n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you have an example of updated topic assignment per word in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1]),\n",
       "       list([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]),\n",
       "       list([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]),\n",
       "       list([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]),\n",
       "       list([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]),\n",
       "       list([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And TA DA here's the updated probability matrix!  Row represents text, columns represents topic! Based on this output for all texts we can do clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19319787, 0.44964463, 0.02829765, 0.22951366, 0.09934619],\n",
       "       [0.68091444, 0.02889871, 0.11734515, 0.02921026, 0.14363143],\n",
       "       [0.44619695, 0.11603124, 0.07688231, 0.21371723, 0.14717228],\n",
       "       [0.04372327, 0.15736951, 0.05675589, 0.38374274, 0.35840859],\n",
       "       [0.09095947, 0.26223147, 0.215264  , 0.30168737, 0.12985769],\n",
       "       [0.3201997 , 0.17479029, 0.24561528, 0.12654884, 0.13284589]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
